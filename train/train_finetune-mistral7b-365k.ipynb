{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning Mistral 7B - Conversational with MetaMathQA_395k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 0: C√†i th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:51:18.543595Z",
     "iopub.status.busy": "2025-06-30T01:51:18.543446Z",
     "iopub.status.idle": "2025-06-30T01:53:58.940138Z",
     "shell.execute_reply": "2025-06-30T01:53:58.939144Z",
     "shell.execute_reply.started": "2025-06-30T01:51:18.543579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install unsloth vllm\n",
    "# !pip install --upgrade transformers==4.52.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 0: ƒêƒÉng nh·∫≠p HuggingFace ƒë·ªÉ l·∫•y m√¥ h√¨nh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:53:58.942891Z",
     "iopub.status.busy": "2025-06-30T01:53:58.942616Z",
     "iopub.status.idle": "2025-06-30T01:53:59.636943Z",
     "shell.execute_reply": "2025-06-30T01:53:59.636328Z",
     "shell.execute_reply.started": "2025-06-30T01:53:58.942867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN] [--add-to-git-credential]\n",
      "huggingface-cli <command> [<args>] login: error: argument --token: expected one argument\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:53:59.638044Z",
     "iopub.status.busy": "2025-06-30T01:53:59.637823Z",
     "iopub.status.idle": "2025-06-30T01:53:59.771115Z",
     "shell.execute_reply": "2025-06-30T01:53:59.770484Z",
     "shell.execute_reply.started": "2025-06-30T01:53:59.638021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:53:59.772135Z",
     "iopub.status.busy": "2025-06-30T01:53:59.771893Z",
     "iopub.status.idle": "2025-06-30T01:54:00.284767Z",
     "shell.execute_reply": "2025-06-30T01:54:00.284257Z",
     "shell.execute_reply.started": "2025-06-30T01:53:59.772110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=hf_token, new_session=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 1: Load m√¥ h√¨nh Mistral 4bit t·ª´ Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:54:00.285904Z",
     "iopub.status.busy": "2025-06-30T01:54:00.285458Z",
     "iopub.status.idle": "2025-06-30T01:57:15.744169Z",
     "shell.execute_reply": "2025-06-30T01:57:15.743587Z",
     "shell.execute_reply.started": "2025-06-30T01:54:00.285879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 01:54:22.342840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751248462.730696      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751248462.846832      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 06-30 01:54:46 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 06-30 01:54:46 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.6.8: Fast Mistral patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/mistral-7b-instruct-v0.3-bnb-4bit with actual GPU utilization = 49.53%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
      "Unsloth: vLLM's KV Cache can use up to 2.58 GB. Also swap space = 4 GB.\n",
      "WARNING 06-30 01:54:55 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 06-30 01:55:09 [config.py:717] This model supports multiple tasks: {'embed', 'score', 'generate', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 06-30 01:55:09 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\n",
      "INFO 06-30 01:55:09 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/mistral-7b-instruct-v0.3-bnb-4bit', speculative_config=None, tokenizer='unsloth/mistral-7b-instruct-v0.3-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/mistral-7b-instruct-v0.3-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b04579be2264ead8f14e687d458c707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fd0a1f8f3c495c806ff4e9e8c8d53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3a7df79de94620bf9286db8f6004f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a329abe2d7cb48479379c6cfb53f3c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4ee46e64e84bdc9d5b08a74a63ab51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/157 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 01:55:11 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 06-30 01:55:11 [cuda.py:289] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W630 01:55:22.138043669 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 01:55:32 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-30 01:55:32 [model_runner.py:1108] Starting to load model unsloth/mistral-7b-instruct-v0.3-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W630 01:55:32.148739964 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 01:55:32 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 06-30 01:55:32 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfebc40cfc7540e98ca668d125a538d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 01:55:54 [weight_utils.py:281] Time spent downloading weights for unsloth/mistral-7b-instruct-v0.3-bnb-4bit: 21.151145 seconds\n",
      "INFO 06-30 01:55:54 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426fe93dd53b4c21a1d42c00aa8b2ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8c9cb97b144b059ef37ce97c870728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 01:56:05 [logger.py:57] Using PunicaWrapperGPU.\n",
      "INFO 06-30 01:56:06 [model_runner.py:1140] Model loading took 4.0423 GiB and 33.243225 seconds\n",
      "INFO 06-30 01:56:15 [worker.py:287] Memory profiling takes 8.78 seconds\n",
      "INFO 06-30 01:56:15 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.50) = 7.30GiB\n",
      "INFO 06-30 01:56:15 [worker.py:287] model weights take 4.04GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.33GiB; the rest of the memory reserved for KV Cache is 2.90GiB.\n",
      "INFO 06-30 01:56:15 [executor_base.py:112] # cuda blocks: 1486, # CPU blocks: 2048\n",
      "INFO 06-30 01:56:15 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 11.61x\n",
      "INFO 06-30 01:56:19 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171531b488434ee594f1101b67a37344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-30 01:57:08 [model_runner.py:1592] Graph capturing finished in 49 secs, took 0.53 GiB\n",
      "INFO 06-30 01:57:08 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 62.20 seconds\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['k_norm', 'q_norm', 'pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd43f3692634270bc916209f2889f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f8334b3c549dfad9359ace100e856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fb3804fb324f0f8c0c8fa126aa90d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba6cfc1b2974ff3836bb52b93dc7f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Chi·ªÅu d√†i t·ªëi ƒëa 1 ƒëo·∫°n vƒÉn b·∫£n\n",
    "dtype = None           # T·ª± ƒë·ªông ch·ªçn (float16, bfloat16) theo GPU\n",
    "load_in_4bit = True    # D√πng m√¥ h√¨nh l∆∞·ª£ng t·ª≠ h√≥a 4-bit\n",
    "lora_rank = 32\n",
    "\n",
    "# Load model ƒë√£ quant h√≥a 4bit + t·ª± ƒë·ªông chia l√™n GPU ph√π h·ª£p\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 2: √Åp d·ª•ng LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:59:06.964372Z",
     "iopub.status.busy": "2025-06-30T01:59:06.963529Z",
     "iopub.status.idle": "2025-06-30T01:59:06.972223Z",
     "shell.execute_reply": "2025-06-30T01:59:06.971341Z",
     "shell.execute_reply.started": "2025-06-30T01:59:06.964333Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B∆∞·ªõc 3: G√°n Chat Template ki·ªÉu ChatML. Load v√† x·ª≠ l√Ω d·ªØ li·ªáu MetaMathQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:59:12.094963Z",
     "iopub.status.busy": "2025-06-30T01:59:12.094650Z",
     "iopub.status.idle": "2025-06-30T01:59:26.025091Z",
     "shell.execute_reply": "2025-06-30T01:59:26.024509Z",
     "shell.execute_reply.started": "2025-06-30T01:59:12.094941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9626627674004f2f8d5af115273a1d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfa6a7b6f3b4ff2b13401f803bcc18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetaMathQA-395K.json:   0%|          | 0.00/396M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8db9bc2d20499fb9d503ea15d332d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/395000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"meta-math/MetaMathQA\", split=\"train\")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:59:31.317601Z",
     "iopub.status.busy": "2025-06-30T01:59:31.317013Z",
     "iopub.status.idle": "2025-06-30T01:59:31.410080Z",
     "shell.execute_reply": "2025-06-30T01:59:31.409531Z",
     "shell.execute_reply.started": "2025-06-30T01:59:31.317571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:59:33.062868Z",
     "iopub.status.busy": "2025-06-30T01:59:33.062250Z",
     "iopub.status.idle": "2025-06-30T01:59:33.066333Z",
     "shell.execute_reply": "2025-06-30T01:59:33.065565Z",
     "shell.execute_reply.started": "2025-06-30T01:59:33.062845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format d·∫°ng h·ªôi tho·∫°i v·ªõi system instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:59:37.599527Z",
     "iopub.status.busy": "2025-06-30T01:59:37.599224Z",
     "iopub.status.idle": "2025-06-30T01:59:37.914362Z",
     "shell.execute_reply": "2025-06-30T01:59:37.913558Z",
     "shell.execute_reply.started": "2025-06-30T01:59:37.599505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = </s>.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# G√°n template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",  # ho·∫∑c \"mistral\", \"unsloth\"\n",
    "    map_eos_token=True,\n",
    ")\n",
    "\n",
    "# System Instruction\n",
    "system_instruction_fixed = \"\"\"Below is an instruction that describes a mathematical task.\n",
    "Write a response that thoroughly solves the given problem.\n",
    "Before solving, develop a clear step-by-step chain of reasoning to ensure accuracy and logical coherence.\n",
    "\n",
    "### Instruction:\n",
    "You are a mathematics expert with advanced knowledge in mathematical reasoning, problem-solving, and proof techniques. You think outloud and consider various aspects before giving any concrete answers.\"\"\"\n",
    "\n",
    "def formatting_prompts_func_conversational_structured(examples):\n",
    "    queries = examples[\"query\"]\n",
    "    responses = examples[\"response\"]\n",
    "    texts = []\n",
    "\n",
    "    for q, r in zip(queries, responses):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_instruction_fixed},\n",
    "            {\"role\": \"user\", \"content\": q},\n",
    "            {\"role\": \"assistant\", \"content\": r},\n",
    "        ]\n",
    "        formatted = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(formatted)\n",
    "\n",
    "    return {\"text\": texts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T01:59:43.511855Z",
     "iopub.status.busy": "2025-06-30T01:59:43.511470Z",
     "iopub.status.idle": "2025-06-30T02:00:06.715435Z",
     "shell.execute_reply": "2025-06-30T02:00:06.714573Z",
     "shell.execute_reply.started": "2025-06-30T01:59:43.511825Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4440c5ea3014caaab38ff7169a85769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/316000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a0b19a1cbf4a96aa2c42bd6fd97fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/79000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(formatting_prompts_func_conversational_structured, batched=True)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func_conversational_structured, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Tokenize\n",
    "# def tokenize_func(example):\n",
    "#     tokenized = tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         truncation=True,\n",
    "#         max_length=2048,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "#     return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tokenized_train = train_dataset.map(tokenize_func, batched=False)\n",
    "# tokenized_eval = eval_dataset.map(tokenize_func, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# # 1. G√°n template ƒë√∫ng ki·ªÉu Mistral Instruct\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"mistral\",  # CHU·∫®N CHO MISTRAL INSTRUCT V0.3\n",
    "#     mapping = {\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "#     map_eos_token = True,\n",
    "# )\n",
    "\n",
    "# # 2. T·∫£i v√† ch·ªçn ph·∫ßn d·ªØ li·ªáu t·ª´ MetaMathQA\n",
    "# dataset = load_dataset(\"meta-math/MetaMathQA\", split=\"train\")\n",
    "# dataset = dataset.select(range(100000, 200000))  # L·∫•y t·ª´ d√≤ng 100001 ƒë·∫øn 200000\n",
    "# print(\"S·ªë m·∫´u:\", len(dataset))\n",
    "\n",
    "# # 3. Chuy·ªÉn ƒë·ªïi th√†nh ƒë·ªãnh d·∫°ng h·ªôi tho·∫°i ShareGPT-style\n",
    "# def convert_to_conversation(example):\n",
    "#     return {\n",
    "#         \"conversations\": [\n",
    "#             {\"from\": \"human\", \"value\": example[\"query\"]},\n",
    "#             {\"from\": \"gpt\", \"value\": example[\"response\"]},\n",
    "#         ]\n",
    "#     }\n",
    "\n",
    "# dataset = dataset.map(convert_to_conversation)\n",
    "\n",
    "# # 4. Format theo chat_template ƒë√£ g√°n\n",
    "# def formatting_prompts_func(examples):\n",
    "#     texts = [\n",
    "#         tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
    "#         for convo in examples[\"conversations\"]\n",
    "#     ]\n",
    "#     return {\"text\": texts}\n",
    "\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# # 5. Tokenize\n",
    "# def tokenize_func(example):\n",
    "#     tokenized = tokenizer(\n",
    "#         example[\"text\"],\n",
    "#         truncation=True,\n",
    "#         max_length=2048,\n",
    "#         padding=\"max_length\",\n",
    "#     )\n",
    "#     tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "#     return tokenized\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_func, batched=False)\n",
    "\n",
    "# # K·∫øt qu·∫£: tokenized_dataset s·∫µn s√†ng ƒë·ªÉ hu·∫•n luy·ªán v·ªõi SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 5: Hu·∫•n luy·ªán b·∫±ng `SFTTrainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = dataset,             # Dataset ch·ª©a field \"text\"\n",
    "#     dataset_text_field = \"text\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     dataset_num_proc = 2,\n",
    "#     packing = False,                     # ƒê·ªÉ True n·∫øu c√°c m·∫´u ng·∫Øn\n",
    "#     args = SFTConfig(\n",
    "#         output_dir = \"outputs\",\n",
    "#         per_device_train_batch_size = 2,\n",
    "#         gradient_accumulation_steps = 4,\n",
    "#         warmup_steps = 5,\n",
    "#         max_steps = 60,                  # Ho·∫∑c d√πng num_train_epochs=1\n",
    "#         learning_rate = 1e-4,\n",
    "#         optim = \"adamw_8bit\",            # T·ªëi ∆∞u cho 4bit\n",
    "#         weight_decay = 0.01,\n",
    "#         lr_scheduler_type = \"linear\",\n",
    "#         logging_steps = 1,\n",
    "#         report_to = \"none\",\n",
    "#         seed = 3407,\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:00:31.695207Z",
     "iopub.status.busy": "2025-06-30T02:00:31.694451Z",
     "iopub.status.idle": "2025-06-30T02:02:39.806765Z",
     "shell.execute_reply": "2025-06-30T02:02:39.806045Z",
     "shell.execute_reply.started": "2025-06-30T02:00:31.695183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7749c8cd3c0a42999899a06c55a1f63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/316000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9912b69a174f458d8fd57d34be927f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/79000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Kh·ªüi t·∫°o Trainer ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi c√°c thi·∫øt l·∫≠p ƒë√£ t·ªëi ∆∞u cho to√°n h·ªçc\n",
    "trainer = SFTTrainer(\n",
    "    model = model,                          # M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã (c√≥ th·ªÉ LoRA ho·∫∑c fine-tuned)\n",
    "    tokenizer = tokenizer,                  # Tokenizer ph√π h·ª£p v·ªõi m√¥ h√¨nh\n",
    "    train_dataset = train_dataset,        # D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c token h√≥a cho hu·∫•n luy·ªán\n",
    "    eval_dataset = eval_dataset,          # D·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° trong qu√° tr√¨nh train\n",
    "    dataset_text_field = \"text\",            # Tr∆∞·ªùng ch·ª©a chu·ªói ƒë·∫ßu v√†o\n",
    "    max_seq_length = 2048,                  # ƒê·ªô d√†i t·ªëi ƒëa m·ªói chu·ªói, ph√π h·ª£p v·ªõi Mistral\n",
    "    dataset_num_proc = 2,                   # S·ªë ti·∫øn tr√¨nh song song ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "    packing = False,                        # Kh√¥ng g·ªôp nhi·ªÅu m·∫´u ng·∫Øn l·∫°i ‚Äì t·ªët h∆°n cho to√°n h·ªçc v√¨ m·∫´u th∆∞·ªùng d√†i\n",
    "    args = SFTConfig(\n",
    "        output_dir = \"outputs\",             # Th∆∞ m·ª•c ƒë·ªÉ l∆∞u checkpoint v√† log\n",
    "        per_device_train_batch_size = 2,    # Batch size m·ªói GPU ‚Äì tƒÉng n·∫øu ƒë·ªß VRAM\n",
    "        per_device_eval_batch_size = 2,     # Batch size khi ƒë√°nh gi√° ‚Äì kh√¥ng ·∫£nh h∆∞·ªüng t·ªëc ƒë·ªô train\n",
    "        gradient_accumulation_steps = 4,    # S·ªë b∆∞·ªõc t√≠ch l≈©y gradient ‚Äì tƒÉng n·∫øu VRAM h·∫°n ch·∫ø\n",
    "        warmup_steps = 60,                  # S·ªë b∆∞·ªõc ƒë·∫ßu warmup learning rate (5% ~ 1000 steps) => S·ªë ch√≠nh x√°c: 60\n",
    "        max_steps = 1200,                   # T·ªïng s·ªë b∆∞·ªõc hu·∫•n luy·ªán (b·∫°n c√≥ th·ªÉ gi·∫£m n·∫øu c·∫ßn) => S·ªë ch√≠nh x√°c: 1200\n",
    "        learning_rate = 2e-4,               # Learning rate ‚Äì th∆∞·ªùng d√πng 2e-4 cho LoRA\n",
    "        optim = \"adamw_8bit\",               # Optimizer nh·∫π, ph√π h·ª£p v·ªõi m√¥ h√¨nh 4-bit\n",
    "        weight_decay = 0.01,                # Tr√°nh overfitting nh·∫π\n",
    "        lr_scheduler_type = \"cosine\",       # TƒÉng gi·∫£m learning rate m∆∞·ª£t m√† h∆°n ‚Äúlinear‚Äù\n",
    "        logging_steps = 10,                 # In log sau m·ªói 10 b∆∞·ªõc\n",
    "        report_to = \"none\",                 # N·∫øu mu·ªën theo d√µi b·∫±ng wandb th√¨ ƒë·ªïi th√†nh \"wandb\"\n",
    "        seed = 3407,                        # ƒê·∫£m b·∫£o k·∫øt qu·∫£ reproducible n·∫øu c·∫ßn\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T02:03:08.292057Z",
     "iopub.status.busy": "2025-06-30T02:03:08.291691Z",
     "iopub.status.idle": "2025-06-30T09:11:19.021997Z",
     "shell.execute_reply": "2025-06-30T09:11:19.021402Z",
     "shell.execute_reply.started": "2025-06-30T02:03:08.292030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 316,000 | Num Epochs = 1 | Total steps = 1,200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 83,886,080/7,000,000,000 (1.20% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 7:07:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.374900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.355200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.332900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.339500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.337700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.355700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.324400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.331800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.308900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.294400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.297600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.267900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.282900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.281700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.272000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.279100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.260700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.260500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.268500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:14:34.963826Z",
     "iopub.status.busy": "2025-06-30T09:14:34.963218Z",
     "iopub.status.idle": "2025-06-30T09:14:34.968370Z",
     "shell.execute_reply": "2025-06-30T09:14:34.967692Z",
     "shell.execute_reply.started": "2025-06-30T09:14:34.963800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=0.3191232430934906, metrics={'train_runtime': 25687.4702, 'train_samples_per_second': 0.747, 'train_steps_per_second': 0.047, 'total_flos': 2.17223099365589e+17, 'train_loss': 0.3191232430934906})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 7: Inference sau khi hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:14:38.581609Z",
     "iopub.status.busy": "2025-06-30T09:14:38.581256Z",
     "iopub.status.idle": "2025-06-30T09:14:38.873886Z",
     "shell.execute_reply": "2025-06-30T09:14:38.873291Z",
     "shell.execute_reply.started": "2025-06-30T09:14:38.581590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "import torch\n",
    "\n",
    "# K√≠ch ho·∫°t ch·∫ø ƒë·ªô inference nhanh\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# G·∫Øn l·∫°i chat template chu·∫©n chatml (ho·∫∑c mistral n·∫øu fine-tune theo ki·ªÉu ƒë√≥)\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"chatml\",\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
    "    map_eos_token=True,\n",
    ")\n",
    "\n",
    "# H√†m h·ªèi ƒë√°p inference\n",
    "def chat(question, max_new_tokens=512, stream=False):\n",
    "    messages = [{\"from\": \"human\", \"value\": question}]\n",
    "    \n",
    "    # T·∫°o prompt theo chu·∫©n chat\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # T√πy ch·ªçn stream ra m√†n h√¨nh tr·ª±c ti·∫øp\n",
    "    if stream:\n",
    "        streamer = TextStreamer(tokenizer)\n",
    "        _ = model.generate(\n",
    "            input_ids=inputs,\n",
    "            streamer=streamer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            top_k=1,\n",
    "            repetition_penalty=1.1,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    else:\n",
    "        # Sinh ra k·∫øt qu·∫£ ho√†n ch·ªânh\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            top_p=1.0,\n",
    "            top_k=1,\n",
    "            repetition_penalty=1.1,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Gi·∫£i m√£ k·∫øt qu·∫£\n",
    "        result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "        # C·∫Øt ph·∫ßn prompt n·∫øu c·∫ßn\n",
    "        response = result.split(question.strip())[-1].strip()\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:16:23.464967Z",
     "iopub.status.busy": "2025-06-30T09:16:23.464230Z",
     "iopub.status.idle": "2025-06-30T09:16:39.617668Z",
     "shell.execute_reply": "2025-06-30T09:16:39.616895Z",
     "shell.execute_reply.started": "2025-06-30T09:16:23.464939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>assistant\n",
      "We want to find the value of $X$ in the given situation.\n",
      "We are given that $x - y = X$ and $x + y = 12$.\n",
      "To find the value of $x$, we can solve this system of equations.\n",
      "We can start by solving the second equation for $y$:\n",
      "$x + y = 12$\n",
      "Subtracting $x$ from both sides of the equation, we get:\n",
      "$y = 12 - x$\n",
      "Now, substitute this value of $y$ into the first equation:\n",
      "$x - (12 - x) = X$\n",
      "Simplifying, we have:\n",
      "$-x + 12 = X$\n",
      "Adding $x$ to both sides of the equation, we find:\n",
      "$12 = X + x$\n",
      "Given that the value of $x$ is 9, we can substitute it into the equation:\n",
      "$12 = X + 9$\n",
      "Subtracting 9 from both sides of the equation, we get:\n",
      "$3 = X$\n",
      "The value of X is 3.\n",
      "The answer is: 3\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"If $x - y = X and $x + y = 12$, what is the value of $x$? If we know the answer to the above question is 9, what is the value of unknown variable X?\")\n",
    "print(response) # ƒê√°p √°n: 6 => Sai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:16:44.525477Z",
     "iopub.status.busy": "2025-06-30T09:16:44.524805Z",
     "iopub.status.idle": "2025-06-30T09:16:57.396781Z",
     "shell.execute_reply": "2025-06-30T09:16:57.396223Z",
     "shell.execute_reply.started": "2025-06-30T09:16:44.525446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "A radio show plays for 3 hours a day. They split their show into talking segments, ad breaks and songs. Talking segments last 10 minutes each, ad breaks last 5 minutes each and songs are played throughout the rest of the show. If the radio show includes 3 talking segments and x ad breaks in today‚Äôs show, how long, in minutes, does the show play songs? If we know the answer to the above question is 125, what is the value of unknown variable x?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The radio show plays for 3 hours, which is equal to 3 * 60 = 180 minutes.\n",
      "Talking segments last 10 minutes each, so 3 talking segments will last 3 * 10 = 30 minutes.\n",
      "Ad breaks last 5 minutes each, so x ad breaks will last x * 5 = 5x minutes.\n",
      "The remaining time is spent playing songs, so the total time spent on songs is 180 - 30 - 5x = 150 - 5x minutes.\n",
      "We are given that the total time spent on songs is 125 minutes, so we can write: 150 - 5x = 125.\n",
      "Solving for x, we get: x = 5.\n",
      "The value of x is 5.\n",
      "#### 5\n",
      "The answer is: 5<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"A radio show plays for 3 hours a day. They split their show into talking segments, ad breaks and songs. Talking segments last 10 minutes each, ad breaks last 5 minutes each and songs are played throughout the rest of the show. If the radio show includes 3 talking segments and x ad breaks in today‚Äôs show, how long, in minutes, does the show play songs? If we know the answer to the above question is 125, what is the value of unknown variable x?\", stream=True)\n",
    "# ƒê√∫ng, ƒë√°p = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:17:09.366168Z",
     "iopub.status.busy": "2025-06-30T09:17:09.365869Z",
     "iopub.status.idle": "2025-06-30T09:17:15.745225Z",
     "shell.execute_reply": "2025-06-30T09:17:15.744585Z",
     "shell.execute_reply.started": "2025-06-30T09:17:09.366148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>assistant\n",
      "Ten students raised $20 each, so they contributed 10 * $20 = $200.\n",
      "The remaining twenty students raised $30 each, so they contributed 20 * $30 = $600.\n",
      "Therefore, the total amount of money raised by the students is $200 + $600 = $800.\n",
      "#### 800\n",
      "The answer is: 800\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"In a charity race to raise money for hurricane victims, thirty students participated. Ten of them raised $20 each, while the remaining students raised $30 each. What is the total amount of money raised by the students in the race?\")\n",
    "print(response)\n",
    "# ƒê√∫ng, ƒë√°p √°n 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:17:51.984099Z",
     "iopub.status.busy": "2025-06-30T09:17:51.983469Z",
     "iopub.status.idle": "2025-06-30T09:17:56.545178Z",
     "shell.execute_reply": "2025-06-30T09:17:56.544525Z",
     "shell.execute_reply.started": "2025-06-30T09:17:51.984071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Which two-digit positive integer is one more than a multiple of 2, 3, 4, 5, and 6?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The smallest two-digit positive integer that is one more than a multiple of 2, 3, 4, 5, and 6 is $10$.\n",
      "Since $10$ is divisible by all the given numbers, it is the only possible answer.\n",
      "The answer is: 10<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"Which two-digit positive integer is one more than a multiple of 2, 3, 4, 5, and 6?\", stream=True)\n",
    "# Sai ho√†n to√†n, ƒë√°p l√†: 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:18:00.180434Z",
     "iopub.status.busy": "2025-06-30T09:18:00.179734Z",
     "iopub.status.idle": "2025-06-30T09:18:03.788976Z",
     "shell.execute_reply": "2025-06-30T09:18:03.788461Z",
     "shell.execute_reply.started": "2025-06-30T09:18:00.180412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Simplify: $|{-3^2+4}|$<|im_end|>\n",
      "<|im_start|>assistant\n",
      "We can simplify the expression inside the absolute value first.\n",
      "$-3^2 + 4 = -9 + 4 = -5$.\n",
      "So, $|-5| = \\boxed{5}$.The answer is: 5<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"Simplify: $|{-3^2+4}|$\", stream=True)\n",
    "# Sai, ƒë√°p: 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:18:08.708606Z",
     "iopub.status.busy": "2025-06-30T09:18:08.707894Z",
     "iopub.status.idle": "2025-06-30T09:18:24.295910Z",
     "shell.execute_reply": "2025-06-30T09:18:24.295405Z",
     "shell.execute_reply.started": "2025-06-30T09:18:08.708581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Steven's teacher sends the class an assignment to collect x different fruit seeds. Apples average 6 seeds, pears average 2 seeds, and grapes average 3 seeds. Steven has set aside 4 apples, 3 pears, and 9 grapes to extract their seeds. How many more seeds does he need to fulfill his assignment? If we know the answer to the above question is 3, what is the value of unknown variable x?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The number of seeds in each type of fruit can be calculated as follows:\n",
      "Number of seeds in apples = Number of apples * Average number of seeds per apple = 4 * 6 = 24\n",
      "Number of seeds in pears = Number of pears * Average number of seeds per pear = 3 * 2 = 6\n",
      "Number of seeds in grapes = Number of grapes * Average number of seeds per grape = 9 * 3 = 27\n",
      "The total number of seeds Steven has set aside is the sum of the seeds in each type of fruit: 24 + 6 + 27 = 57\n",
      "We are given that Steven needs to collect x different fruit seeds, so the number of additional seeds he needs is: x - 57\n",
      "We are also given that the number of additional seeds needed is 3, so we can write: x - 57 = 3\n",
      "Solving for x, we get: x = 60\n",
      "The value of x is 60.\n",
      "#### 60\n",
      "The answer is: 60<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"Steven's teacher sends the class an assignment to collect x different fruit seeds. Apples average 6 seeds, pears average 2 seeds, and grapes average 3 seeds. Steven has set aside 4 apples, 3 pears, and 9 grapes to extract their seeds. How many more seeds does he need to fulfill his assignment? If we know the answer to the above question is 3, what is the value of unknown variable x?\", stream=True)\n",
    "# ƒê√°p √°n 60 => ƒê√∫ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:18:35.688365Z",
     "iopub.status.busy": "2025-06-30T09:18:35.687752Z",
     "iopub.status.idle": "2025-06-30T09:18:50.184220Z",
     "shell.execute_reply": "2025-06-30T09:18:50.183715Z",
     "shell.execute_reply.started": "2025-06-30T09:18:35.688340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "I am a two-digit number.\n",
      "The sum of my digits is 11.\n",
      "If you reverse my digits, the new number = the original number + 27.\n",
      "What number am I?\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let's call the two-digit number $AB$.\n",
      "We are given that $A+B=11$ and $AB+27=BA+27$.\n",
      "Subtracting these equations, we get $(A-B)(B-A)=0$.\n",
      "Since $A\\neq B$, we must have $A=B$.\n",
      "Therefore, $AB=AA=A^2$.\n",
      "We know that $A^2+27=BA+27=11A$.\n",
      "Substituting $A=B$, we get $A^2+27=11A$.\n",
      "Rearranging, we have $A^2-11A+27=0$.\n",
      "Factoring, we find $(A-3)(A-9)=0$.\n",
      "Since $A$ is a two-digit number, we must have $A=3$.\n",
      "Therefore, the two-digit number is $\\boxed{33}$.\n",
      "The answer is: 33<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\n",
    "\"\"\"I am a two-digit number.\n",
    "The sum of my digits is 11.\n",
    "If you reverse my digits, the new number = the original number + 27.\n",
    "What number am I?\n",
    "\"\"\",\n",
    "    stream=True\n",
    ")\n",
    "# ƒê√°p √°n: 47 => Sai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:18:55.837480Z",
     "iopub.status.busy": "2025-06-30T09:18:55.837187Z",
     "iopub.status.idle": "2025-06-30T09:19:17.392796Z",
     "shell.execute_reply": "2025-06-30T09:19:17.392224Z",
     "shell.execute_reply.started": "2025-06-30T09:18:55.837460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "At Frank's Fruit Market, 3 bananas cost as much as 2 apples, and 6 apples cost as much as 4 oranges. How many oranges cost as much as 18 bananas?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Let $b$ be the price of one banana, $a$ be the price of one apple, and $o$ be the price of one orange.\n",
      "From the given information, we have the following equations:\n",
      "\\[3b = 2a\\]\n",
      "\\[6a = 4o\\]\n",
      "We can solve these equations to find the values of $b$, $a$, and $o$.\n",
      "First, we can solve the first equation for $a$:\n",
      "\\[3b = 2a \\Rightarrow a = \\frac{3}{2}b\\]\n",
      "Next, we can substitute this value of $a$ into the second equation:\n",
      "\\[\\frac{3}{2}b = 4o \\Rightarrow b = \\frac{8}{3}o\\]\n",
      "Now, we can use either equation to find the price of one banana in terms of the price of one orange:\n",
      "\\[b = \\frac{8}{3}o \\Rightarrow o = \\frac{3}{8}b\\]\n",
      "Finally, we can use this expression for $o$ to find the number of oranges that cost as much as 18 bananas:\n",
      "\\[\\frac{3}{8}b \\cdot 18 = 9b\\]\n",
      "Since $b$ is the price of one banana, we can rewrite this equation as:\n",
      "\\[9b = 9b\\]\n",
      "This equation holds true for any value of $b$, so the answer is $\\boxed{9}$.The answer is: 9<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"At Frank's Fruit Market, 3 bananas cost as much as 2 apples, and 6 apples cost as much as 4 oranges. How many oranges cost as much as 18 bananas?\", stream=True)\n",
    "# Sai, ƒë√°p √°n l√† 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:19:41.004984Z",
     "iopub.status.busy": "2025-06-30T09:19:41.004401Z",
     "iopub.status.idle": "2025-06-30T09:19:48.630870Z",
     "shell.execute_reply": "2025-06-30T09:19:48.630317Z",
     "shell.execute_reply.started": "2025-06-30T09:19:41.004963Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Jill bought 5 packs of red bouncy balls and 4 packs of yellow bouncy balls. Each package contained 18 bouncy balls. How many more red bouncy balls than yellow bouncy balls did Jill buy?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Jill bought 5 packs x 18 = <<5*18=90>>90 red bouncy balls.\n",
      "She also bought 4 packs x 18 = <<4*18=72>>72 yellow bouncy balls.\n",
      "Therefore, she bought 90 - 72 = <<90-72=18>>18 more red bouncy balls than yellow bouncy balls.\n",
      "#### 18\n",
      "The answer is: 18<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"Jill bought 5 packs of red bouncy balls and 4 packs of yellow bouncy balls. Each package contained 18 bouncy balls. How many more red bouncy balls than yellow bouncy balls did Jill buy?\", stream=True)\n",
    "# ƒê√∫ng, ƒë√°p √°n l√† 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:20:34.501532Z",
     "iopub.status.busy": "2025-06-30T09:20:34.500762Z",
     "iopub.status.idle": "2025-06-30T09:20:43.395074Z",
     "shell.execute_reply": "2025-06-30T09:20:43.394477Z",
     "shell.execute_reply.started": "2025-06-30T09:20:34.501506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "A 1$\\%$ late charge was added to Jenna's bill on the $30^{\text{th}}$ day past its due date. The resulting total was then increased by 1$\\%$ because she did not pay the bill in the next 30 days either. Her original bill was $\\$400$. Exactly how much is the bill now?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The first late fee of 1% of the bill amount, which is $400 \\times 1\\% = \\$4$, was added on the 30th day past the due date.\n",
      "After this, the bill amount became $400 + 4 = \\$404$.\n",
      "On the next 30 days, another 1% late fee was added, which is $404 \\times 1\\% = \\$4$.\n",
      "So, the final bill amount is $404 + 4 = \\boxed{\\$408}$.\n",
      "The answer is: 408<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"A 1$\\%$ late charge was added to Jenna's bill on the $30^{\\text{th}}$ day past its due date. The resulting total was then increased by 1$\\%$ because she did not pay the bill in the next 30 days either. Her original bill was $\\$400$. Exactly how much is the bill now?\", stream=True)\n",
    "# Sai, ƒë√°p √°n l√† 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:22:52.567144Z",
     "iopub.status.busy": "2025-06-30T09:22:52.566649Z",
     "iopub.status.idle": "2025-06-30T09:23:01.187533Z",
     "shell.execute_reply": "2025-06-30T09:23:01.187026Z",
     "shell.execute_reply.started": "2025-06-30T09:22:52.567119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Compute $\\dbinom{16}{5}$.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "We can use the formula for combinations to compute this. The number of ways to choose 5 items from a set of 16 is given by $\\binom{16}{5} = \\frac{16!}{5!(16-5)!} = \\frac{16!}{5!11!} = \\frac{16\\times15\\times14\\times13\\times12}{5\\times4\\times3\\times2\\times1} = \\boxed{4368}$.The answer is: 4368<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat(\"Compute $\\dbinom{16}{5}$.\", stream=True)\n",
    "# ƒê√∫ng, ƒë√°p s·ªë 4368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# questions = [\n",
    "#     \"What is the integral of x^2?\",\n",
    "#     \"Define a group in abstract algebra.\",\n",
    "#     \"Prove that the square root of 2 is irrational.\",\n",
    "# ]\n",
    "\n",
    "# for q in questions:\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         [{\"role\": \"user\", \"content\": q}],\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True,\n",
    "#     )\n",
    "#     output = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.7)\n",
    "#     print(f\"\\n‚ùì {q}\\nüß† {output[0]['generated_text']}\\n\" + \"-\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B∆∞·ªõc 8: L∆∞u m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T·∫£i m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:12:38.742323Z",
     "iopub.status.busy": "2025-06-30T09:12:38.741994Z",
     "iopub.status.idle": "2025-06-30T09:12:39.444535Z",
     "shell.execute_reply": "2025-06-30T09:12:39.443776Z",
     "shell.execute_reply.started": "2025-06-30T09:12:38.742265Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mistral-metamathqa-lora-365k/tokenizer_config.json',\n",
       " 'mistral-metamathqa-lora-365k/special_tokens_map.json',\n",
       " 'mistral-metamathqa-lora-365k/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"mistral-metamathqa-lora-365k\")\n",
    "tokenizer.save_pretrained(\"mistral-metamathqa-lora-365k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:12:41.683250Z",
     "iopub.status.busy": "2025-06-30T09:12:41.682548Z",
     "iopub.status.idle": "2025-06-30T09:12:58.626238Z",
     "shell.execute_reply": "2025-06-30T09:12:58.625618Z",
     "shell.execute_reply.started": "2025-06-30T09:12:41.683226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/mistral-metamathqa-lora-365k.zip'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive(\n",
    "    base_name=\"/kaggle/working/mistral-metamathqa-lora-365k\",  # T√™n file zip (kh√¥ng c·∫ßn .zip ·ªü ƒë√¢y)\n",
    "    format=\"zip\",  # C√≥ th·ªÉ l√†: 'zip', 'tar', 'gztar', 'bztar', 'xztar'\n",
    "    root_dir=\"/kaggle/working/mistral-metamathqa-lora-365k\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load m√¥ h√¨nh l√™n HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:13:04.965802Z",
     "iopub.status.busy": "2025-06-30T09:13:04.965231Z",
     "iopub.status.idle": "2025-06-30T09:13:05.360530Z",
     "shell.execute_reply": "2025-06-30T09:13:05.359964Z",
     "shell.execute_reply.started": "2025-06-30T09:13:04.965778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/hahuy2004/mistral-metamathqa-lora-365k', endpoint='https://huggingface.co', repo_type='model', repo_id='hahuy2004/mistral-metamathqa-lora-365k')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "create_repo(\"mistral-metamathqa-lora-365k\", private=False)  # ho·∫∑c private=False n·∫øu mu·ªën c√¥ng khai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T09:13:12.956116Z",
     "iopub.status.busy": "2025-06-30T09:13:12.955594Z",
     "iopub.status.idle": "2025-06-30T09:13:21.970562Z",
     "shell.execute_reply": "2025-06-30T09:13:21.969958Z",
     "shell.execute_reply.started": "2025-06-30T09:13:12.956097Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0423e2291b0d41b9879418f247c503d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hahuy2004/mistral-metamathqa-lora-365k/commit/7866f26e8a811c440edbbe58f9c30f937cc424f8', commit_message='Upload folder using huggingface_hub', commit_description='', oid='7866f26e8a811c440edbbe58f9c30f937cc424f8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hahuy2004/mistral-metamathqa-lora-365k', endpoint='https://huggingface.co', repo_type='model', repo_id='hahuy2004/mistral-metamathqa-lora-365k'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"/kaggle/working/mistral-metamathqa-lora-365k\",\n",
    "    repo_id=\"hahuy2004/mistral-metamathqa-lora-365k\",  # s·ª≠a l·∫°i ƒë√∫ng username Hugging Face c·ªßa b·∫°n\n",
    "    repo_type=\"model\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
