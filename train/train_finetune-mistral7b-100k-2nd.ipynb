{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12243769,"sourceType":"datasetVersion","datasetId":7714560},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["## Accessing Mistral 7B"],"metadata":{"id":"w4hsXHo6vcSd"}},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"S4SykmKlvcSi"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Trainning Mistral 7B - Conversational with MetaMathQA_395k (L·∫•y d·ªØ li·ªáu 100k th·ª© II)"],"metadata":{"id":"OicriZPJvcSj"}},{"cell_type":"markdown","source":["### B∆∞·ªõc 0: C√†i th∆∞ vi·ªán c·∫ßn thi·∫øt"],"metadata":{"id":"3BH4G5vhvcSj"}},{"cell_type":"code","source":["%%capture\n","!pip install pip3-autoremove\n","!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n","!pip install unsloth vllm\n","# !pip install --upgrade transformers==4.52.3"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:15:41.098570Z","iopub.execute_input":"2025-06-29T05:15:41.098807Z","iopub.status.idle":"2025-06-29T05:18:07.203898Z","shell.execute_reply.started":"2025-06-29T05:15:41.098786Z","shell.execute_reply":"2025-06-29T05:18:07.202533Z"},"id":"ANtcvs7QvcSk"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["### B∆∞·ªõc 0: ƒêƒÉng nh·∫≠p HuggingFace ƒë·ªÉ l·∫•y m√¥ h√¨nh"],"metadata":{"id":"jHV3wjI0vcSm"}},{"cell_type":"code","source":["!huggingface-cli login --token $secret_hf"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:19:12.426747Z","iopub.execute_input":"2025-06-29T05:19:12.427260Z","iopub.status.idle":"2025-06-29T05:19:13.043117Z","shell.execute_reply.started":"2025-06-29T05:19:12.427223Z","shell.execute_reply":"2025-06-29T05:19:13.042460Z"},"id":"utoFYs0SvcSn","outputId":"fe28e5a1-a451-4700-f411-a70ca071cde1"},"outputs":[{"name":"stdout","text":"usage: huggingface-cli <command> [<args>] login [-h] [--token TOKEN] [--add-to-git-credential]\nhuggingface-cli <command> [<args>] login: error: argument --token: expected one argument\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:19:14.319686Z","iopub.execute_input":"2025-06-29T05:19:14.319960Z","iopub.status.idle":"2025-06-29T05:19:14.433551Z","shell.execute_reply.started":"2025-06-29T05:19:14.319933Z","shell.execute_reply":"2025-06-29T05:19:14.433020Z"},"id":"1SAhtOyZvcSp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=hf_token, new_session=False)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:19:15.727782Z","iopub.execute_input":"2025-06-29T05:19:15.728031Z","iopub.status.idle":"2025-06-29T05:19:16.216713Z","shell.execute_reply.started":"2025-06-29T05:19:15.728011Z","shell.execute_reply":"2025-06-29T05:19:16.216000Z"},"id":"OXxqZPldvcSq"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["### B∆∞·ªõc 1: Load m√¥ h√¨nh Mistral 4bit t·ª´ Unsloth"],"metadata":{"id":"Wp05d1UdvcSr"}},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","import torch\n","\n","max_seq_length = 2048  # Chi·ªÅu d√†i t·ªëi ƒëa 1 ƒëo·∫°n vƒÉn b·∫£n\n","dtype = None           # T·ª± ƒë·ªông ch·ªçn (float16, bfloat16) theo GPU\n","load_in_4bit = True    # D√πng m√¥ h√¨nh l∆∞·ª£ng t·ª≠ h√≥a 4-bit\n","lora_rank = 32\n","\n","# Load model ƒë√£ quant h√≥a 4bit + t·ª± ƒë·ªông chia l√™n GPU ph√π h·ª£p\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    fast_inference = True, # Enable vLLM fast inference\n","    max_lora_rank = lora_rank,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:19:18.482425Z","iopub.execute_input":"2025-06-29T05:19:18.482708Z","iopub.status.idle":"2025-06-29T05:22:16.532626Z","shell.execute_reply.started":"2025-06-29T05:19:18.482688Z","shell.execute_reply":"2025-06-29T05:22:16.531830Z"},"colab":{"referenced_widgets":["060a8280be964fd3b6243a8a0ad364c4","ff1d7ac62cc84d92834730c224a8683e","c7059893d775451fa67d879797d8ba33","46b1a62363a44b559e7fee11b82001d4","480075967b364d25ba41bccd88ed5a2f","b5db4260a7b54d7ba8b0ef5f66a3fa7d","5ce99ad5b414442da5494e2280557569","f8ec710944b3415c85f76c8d140acfd1","eaff0958d33b4cbbb86e7260634145d8","ea4fbd2cecdf46b698eb431e497fc116","6f4bf87f1d344fcb975ae848ba651799","27fd2e2505144af7908b5027a625c2a0","6b4799d204ee4e9a8c8f2c4d949b4bc0"]},"id":"kGKf4QV4vcSs","outputId":"9fe101b0-f8fe-4e9c-e01c-6dcdeaea6f89"},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-06-29 05:19:33.561873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751174373.747542      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751174373.803709      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\nINFO 06-29 05:19:51 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 06-29 05:19:51 [__init__.py:239] Automatically detected platform cuda.\n==((====))==  Unsloth 2025.6.8: Fast Mistral patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: vLLM loading unsloth/mistral-7b-instruct-v0.3-bnb-4bit with actual GPU utilization = 49.53%\nUnsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\nUnsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\nUnsloth: vLLM's KV Cache can use up to 2.58 GB. Also swap space = 4 GB.\nWARNING 06-29 05:20:00 [config.py:2972] Casting torch.bfloat16 to torch.float16.\nINFO 06-29 05:20:13 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'classify', 'reward', 'score'}. Defaulting to 'generate'.\nWARNING 06-29 05:20:13 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \nUnsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}\nINFO 06-29 05:20:13 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/mistral-7b-instruct-v0.3-bnb-4bit', speculative_config=None, tokenizer='unsloth/mistral-7b-instruct-v0.3-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/mistral-7b-instruct-v0.3-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"debug\":false,\"dce\":true,\"coordinate_descent_tuning\":true,\"trace.enabled\":false,\"trace.graph_diagram\":false,\"triton.cudagraphs\":true,\"compile_threads\":48,\"max_autotune\":false,\"disable_progress\":false,\"verbose_progress\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060a8280be964fd3b6243a8a0ad364c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1d7ac62cc84d92834730c224a8683e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7059893d775451fa67d879797d8ba33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b1a62363a44b559e7fee11b82001d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/157 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"480075967b364d25ba41bccd88ed5a2f"}},"metadata":{}},{"name":"stdout","text":"INFO 06-29 05:20:16 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 06-29 05:20:16 [cuda.py:289] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"[W629 05:20:26.896184872 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 06-29 05:20:37 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\nINFO 06-29 05:20:37 [model_runner.py:1108] Starting to load model unsloth/mistral-7b-instruct-v0.3-bnb-4bit...\n","output_type":"stream"},{"name":"stderr","text":"[W629 05:20:37.906864277 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 06-29 05:20:37 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\nINFO 06-29 05:20:37 [weight_utils.py:265] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5db4260a7b54d7ba8b0ef5f66a3fa7d"}},"metadata":{}},{"name":"stdout","text":"INFO 06-29 05:20:57 [weight_utils.py:281] Time spent downloading weights for unsloth/mistral-7b-instruct-v0.3-bnb-4bit: 19.694526 seconds\nINFO 06-29 05:20:57 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce99ad5b414442da5494e2280557569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ec710944b3415c85f76c8d140acfd1"}},"metadata":{}},{"name":"stdout","text":"INFO 06-29 05:21:02 [logger.py:57] Using PunicaWrapperGPU.\nINFO 06-29 05:21:02 [model_runner.py:1140] Model loading took 4.0423 GiB and 25.272892 seconds\nINFO 06-29 05:21:16 [worker.py:287] Memory profiling takes 12.92 seconds\nINFO 06-29 05:21:16 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.50) = 7.30GiB\nINFO 06-29 05:21:16 [worker.py:287] model weights take 4.04GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.33GiB; the rest of the memory reserved for KV Cache is 2.90GiB.\nINFO 06-29 05:21:16 [executor_base.py:112] # cuda blocks: 1486, # CPU blocks: 2048\nINFO 06-29 05:21:16 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 11.61x\nINFO 06-29 05:21:20 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Capturing CUDA graph shapes:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaff0958d33b4cbbb86e7260634145d8"}},"metadata":{}},{"name":"stdout","text":"INFO 06-29 05:22:08 [model_runner.py:1592] Graph capturing finished in 49 secs, took 0.53 GiB\nINFO 06-29 05:22:08 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 66.04 seconds\nUnsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'k_norm', 'pre_feedforward_layernorm']\nUnsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'q_norm', 'k_norm', 'pre_feedforward_layernorm']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4fbd2cecdf46b698eb431e497fc116"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f4bf87f1d344fcb975ae848ba651799"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27fd2e2505144af7908b5027a625c2a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4799d204ee4e9a8c8f2c4d949b4bc0"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["### B∆∞·ªõc 2: √Åp d·ª•ng LoRA"],"metadata":{"id":"MxEipkI0vcSt"}},{"cell_type":"code","source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = lora_rank,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:22:37.527567Z","iopub.execute_input":"2025-06-29T05:22:37.528077Z","iopub.status.idle":"2025-06-29T05:22:45.160573Z","shell.execute_reply.started":"2025-06-29T05:22:37.528048Z","shell.execute_reply":"2025-06-29T05:22:45.159816Z"},"id":"xm47fTRvvcSt","outputId":"f5aa90a6-f266-4130-e811-1fab7de19520"},"outputs":[{"name":"stderr","text":"Unsloth 2025.6.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["###  B∆∞·ªõc 3: G√°n Chat Template ki·ªÉu ChatML. Load v√† x·ª≠ l√Ω d·ªØ li·ªáu MetaMathQA"],"metadata":{"id":"pcykCZ_9vcSv"}},{"cell_type":"markdown","source":["### Train/test split"],"metadata":{"id":"PDuPCgjnvcSv"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"meta-math/MetaMathQA\", split=\"train\")\n","dataset = dataset.select(range(100000, 200000))  # Ch·ªâ l·∫•y 100k m·∫´u t·ª´ 100000 - 200000\n","print(len(dataset))"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:23:10.911421Z","iopub.execute_input":"2025-06-29T05:23:10.911724Z","iopub.status.idle":"2025-06-29T05:23:23.991297Z","shell.execute_reply.started":"2025-06-29T05:23:10.911703Z","shell.execute_reply":"2025-06-29T05:23:23.990652Z"},"colab":{"referenced_widgets":["61f299fe2d7f421db9004412d1f571bf","fd1cb67b6ed7448d8b3551def8a01e17","d7fb0e3146304c8782fb41be989c2501"]},"id":"SXO84R6jvcSv","outputId":"f1d8d2f9-b6ce-4910-c846-4bde0f63b19b"},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f299fe2d7f421db9004412d1f571bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"MetaMathQA-395K.json:   0%|          | 0.00/396M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1cb67b6ed7448d8b3551def8a01e17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/395000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7fb0e3146304c8782fb41be989c2501"}},"metadata":{}},{"name":"stdout","text":"100000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["split_dataset = dataset.train_test_split(test_size=0.2, seed=42)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:23:23.992498Z","iopub.execute_input":"2025-06-29T05:23:23.992979Z","iopub.status.idle":"2025-06-29T05:23:24.022853Z","shell.execute_reply.started":"2025-06-29T05:23:23.992959Z","shell.execute_reply":"2025-06-29T05:23:24.022123Z"},"id":"C2Nr8DCIvcSw"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["train_dataset = split_dataset[\"train\"]\n","eval_dataset = split_dataset[\"test\"]"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:23:33.342584Z","iopub.execute_input":"2025-06-29T05:23:33.343211Z","iopub.status.idle":"2025-06-29T05:23:33.346599Z","shell.execute_reply.started":"2025-06-29T05:23:33.343188Z","shell.execute_reply":"2025-06-29T05:23:33.345890Z"},"id":"OWVbHqUyvcSx"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["### Format d·∫°ng h·ªôi tho·∫°i v·ªõi system instruction"],"metadata":{"id":"d-3CRUnLvcSx"}},{"cell_type":"code","source":["from unsloth.chat_templates import get_chat_template\n","\n","# G√°n template\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template=\"chatml\",  # ho·∫∑c \"mistral\", \"unsloth\"\n","    map_eos_token=True,\n",")\n","\n","# System Instruction\n","system_instruction_fixed = \"\"\"Below is an instruction that describes a mathematical task.\n","Write a response that thoroughly solves the given problem.\n","Before solving, develop a clear step-by-step chain of reasoning to ensure accuracy and logical coherence.\n","\n","### Instruction:\n","You are a mathematics expert with advanced knowledge in mathematical reasoning, problem-solving, and proof techniques. You think outloud and consider various aspects before giving any concrete answers.\"\"\"\n","\n","def formatting_prompts_func_conversational_structured(examples):\n","    queries = examples[\"query\"]\n","    responses = examples[\"response\"]\n","    texts = []\n","\n","    for q, r in zip(queries, responses):\n","        messages = [\n","            {\"role\": \"system\", \"content\": system_instruction_fixed},\n","            {\"role\": \"user\", \"content\": q},\n","            {\"role\": \"assistant\", \"content\": r},\n","        ]\n","        formatted = tokenizer.apply_chat_template(\n","            messages,\n","            tokenize=False,\n","            add_generation_prompt=False\n","        )\n","        texts.append(formatted)\n","\n","    return {\"text\": texts}\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:23:45.369000Z","iopub.execute_input":"2025-06-29T05:23:45.369726Z","iopub.status.idle":"2025-06-29T05:23:45.666507Z","shell.execute_reply.started":"2025-06-29T05:23:45.369701Z","shell.execute_reply":"2025-06-29T05:23:45.665895Z"},"id":"NPBL1SglvcSx","outputId":"41317e46-d084-41ba-c326-bab89f41d266"},"outputs":[{"name":"stderr","text":"Unsloth: Will map <|im_end|> to EOS = </s>.\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["train_dataset = train_dataset.map(formatting_prompts_func_conversational_structured, batched=True)\n","eval_dataset = eval_dataset.map(formatting_prompts_func_conversational_structured, batched=True)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:23:52.534995Z","iopub.execute_input":"2025-06-29T05:23:52.535328Z","iopub.status.idle":"2025-06-29T05:23:58.761424Z","shell.execute_reply.started":"2025-06-29T05:23:52.535301Z","shell.execute_reply":"2025-06-29T05:23:58.760453Z"},"colab":{"referenced_widgets":["70d4471d2dfd4f5aa6b230c56c113e5b","84d7105708404c2294fad6c4b7e6c686"]},"id":"wIQ16U0-vcSy","outputId":"ebacf557-e40f-4c5f-c5d5-605d64bb166f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/80000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d4471d2dfd4f5aa6b230c56c113e5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d7105708404c2294fad6c4b7e6c686"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["### Tokenize train and test dataset"],"metadata":{"id":"RjCfOkfBvcSy"}},{"cell_type":"code","source":["# # Tokenize\n","# def tokenize_func(example):\n","#     tokenized = tokenizer(\n","#         example[\"text\"],\n","#         truncation=True,\n","#         max_length=2048,\n","#         padding=\"max_length\",\n","#     )\n","#     tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n","#     return tokenized\n"],"metadata":{"trusted":true,"id":"iWSuE5UhvcSz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# tokenized_train = train_dataset.map(tokenize_func, batched=False)\n","# tokenized_eval = eval_dataset.map(tokenize_func, batched=False)"],"metadata":{"trusted":true,"id":"ovYmM-PivcSz"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["### B∆∞·ªõc 5: Hu·∫•n luy·ªán b·∫±ng `SFTTrainer`"],"metadata":{"id":"fTvgIem6vcS0"}},{"cell_type":"code","source":["from trl import SFTConfig, SFTTrainer\n","\n","# Kh·ªüi t·∫°o Trainer ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi c√°c thi·∫øt l·∫≠p ƒë√£ t·ªëi ∆∞u cho to√°n h·ªçc\n","trainer = SFTTrainer(\n","    model = model,                          # M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c chu·∫©n b·ªã (c√≥ th·ªÉ LoRA ho·∫∑c fine-tuned)\n","    tokenizer = tokenizer,                  # Tokenizer ph√π h·ª£p v·ªõi m√¥ h√¨nh\n","    train_dataset = train_dataset,        # D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c token h√≥a cho hu·∫•n luy·ªán\n","    eval_dataset = eval_dataset,          # D·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° trong qu√° tr√¨nh train\n","    dataset_text_field = \"text\",            # Tr∆∞·ªùng ch·ª©a chu·ªói ƒë·∫ßu v√†o\n","    max_seq_length = 2048,                  # ƒê·ªô d√†i t·ªëi ƒëa m·ªói chu·ªói, ph√π h·ª£p v·ªõi Mistral\n","    dataset_num_proc = 2,                   # S·ªë ti·∫øn tr√¨nh song song ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu\n","    packing = False,                        # Kh√¥ng g·ªôp nhi·ªÅu m·∫´u ng·∫Øn l·∫°i ‚Äì t·ªët h∆°n cho to√°n h·ªçc v√¨ m·∫´u th∆∞·ªùng d√†i\n","    args = SFTConfig(\n","        output_dir = \"outputs\",             # Th∆∞ m·ª•c ƒë·ªÉ l∆∞u checkpoint v√† log\n","        per_device_train_batch_size = 2,    # Batch size m·ªói GPU ‚Äì tƒÉng n·∫øu ƒë·ªß VRAM\n","        per_device_eval_batch_size = 2,     # Batch size khi ƒë√°nh gi√° ‚Äì kh√¥ng ·∫£nh h∆∞·ªüng t·ªëc ƒë·ªô train\n","        gradient_accumulation_steps = 4,    # S·ªë b∆∞·ªõc t√≠ch l≈©y gradient ‚Äì tƒÉng n·∫øu VRAM h·∫°n ch·∫ø\n","        warmup_steps = 60,                  # S·ªë b∆∞·ªõc ƒë·∫ßu warmup learning rate (5% ~ 1000 steps)\n","        max_steps = 1200,                   # T·ªïng s·ªë b∆∞·ªõc hu·∫•n luy·ªán (b·∫°n c√≥ th·ªÉ gi·∫£m n·∫øu c·∫ßn)\n","        learning_rate = 2e-4,               # Learning rate ‚Äì th∆∞·ªùng d√πng 1e-4 cho LoRA\n","        optim = \"adamw_8bit\",               # Optimizer nh·∫π, ph√π h·ª£p v·ªõi m√¥ h√¨nh 4-bit\n","        weight_decay = 0.01,                # Tr√°nh overfitting nh·∫π\n","        lr_scheduler_type = \"cosine\",       # TƒÉng gi·∫£m learning rate m∆∞·ª£t m√† h∆°n ‚Äúlinear‚Äù\n","        logging_steps = 10,                 # In log sau m·ªói 10 b∆∞·ªõc\n","        report_to = \"none\",                 # N·∫øu mu·ªën theo d√µi b·∫±ng wandb th√¨ ƒë·ªïi th√†nh \"wandb\"\n","        seed = 3407,                        # ƒê·∫£m b·∫£o k·∫øt qu·∫£ reproducible n·∫øu c·∫ßn\n","    ),\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:24:52.010504Z","iopub.execute_input":"2025-06-29T05:24:52.010770Z","iopub.status.idle":"2025-06-29T05:25:25.689975Z","shell.execute_reply.started":"2025-06-29T05:24:52.010755Z","shell.execute_reply":"2025-06-29T05:25:25.689403Z"},"colab":{"referenced_widgets":["94749ce173564d70a17f8664df6019b5","557be929de454457a4d7d1bb27f82c60"]},"id":"Hhyc6NuEvcS2","outputId":"ee059072-2621-4913-cf50-19644c4e3841"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/80000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94749ce173564d70a17f8664df6019b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557be929de454457a4d7d1bb27f82c60"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["# B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán\n","trainer.train()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T05:25:29.679627Z","iopub.execute_input":"2025-06-29T05:25:29.680429Z","iopub.status.idle":"2025-06-29T12:32:39.962340Z","shell.execute_reply.started":"2025-06-29T05:25:29.680399Z","shell.execute_reply":"2025-06-29T12:32:39.961577Z"},"id":"Z5qDqmj4vcS2","outputId":"2369e03a-7e08-49a6-ff25-3ac75cd357dd"},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 80,000 | Num Epochs = 1 | Total steps = 1,200\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 83,886,080/7,000,000,000 (1.20% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 7:06:41, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.394700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.707300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.452300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.388600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.379300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.357400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.351700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.360100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.357700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.348700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.356900</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.348600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.352700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.355000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.340200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.336200</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.353300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.343100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.329900</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.334900</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.319000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.337100</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.331400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.342200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.335700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.325000</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.318200</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.328200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.338100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.332100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.329500</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.318600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.318600</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.314300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.306000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.329400</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.320400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.319800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.313100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.326000</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.313300</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.322900</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.316100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.316600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.315200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.325400</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.298900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.315900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.319600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.320800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.304700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.304100</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.310300</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.301100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.306300</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.306900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.297600</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.284200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.301700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.311700</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.292300</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.301300</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.280500</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.309200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.311600</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.310800</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.288900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.311000</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.298800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.298000</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.291600</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.287900</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.282000</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.295800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.300300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.282800</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.298200</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.286900</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.291300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.313800</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.296300</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.296600</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.279800</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.278900</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.286300</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.291800</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.276500</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.281600</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.287000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.272600</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.275300</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.265200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.280900</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.273500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.282400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.289300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.284000</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.263800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.274300</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.262700</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.260500</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.274800</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.291000</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.265800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.268200</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.271600</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.275600</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.264100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.273900</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.259500</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.276700</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.270400</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.263300</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.272400</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.253300</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.293600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.268000</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.282100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.278200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1200, training_loss=0.3185234198967616, metrics={'train_runtime': 25627.2156, 'train_samples_per_second': 0.749, 'train_steps_per_second': 0.047, 'total_flos': 2.1799128472682496e+17, 'train_loss': 0.3185234198967616})"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["### B∆∞·ªõc 7: Inference sau khi hu·∫•n luy·ªán"],"metadata":{"id":"ilO_dV_mvcS3"}},{"cell_type":"code","source":["from unsloth import FastLanguageModel\n","from unsloth.chat_templates import get_chat_template\n","from transformers import TextStreamer\n","import torch\n","\n","# K√≠ch ho·∫°t ch·∫ø ƒë·ªô inference nhanh\n","FastLanguageModel.for_inference(model)\n","\n","# G·∫Øn l·∫°i chat template chu·∫©n chatml (ho·∫∑c mistral n·∫øu fine-tune theo ki·ªÉu ƒë√≥)\n","tokenizer = get_chat_template(\n","    tokenizer,\n","    chat_template=\"chatml\",\n","    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n","    map_eos_token=True,\n",")\n","\n","# H√†m h·ªèi ƒë√°p inference\n","def chat(question, max_new_tokens=512, stream=False):\n","    messages = [{\"from\": \"human\", \"value\": question}]\n","\n","    # T·∫°o prompt theo chu·∫©n chat\n","    inputs = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=True,\n","        add_generation_prompt=True,\n","        return_tensors=\"pt\",\n","    ).to(\"cuda\")\n","\n","    # T√πy ch·ªçn stream ra m√†n h√¨nh tr·ª±c ti·∫øp\n","    if stream:\n","        streamer = TextStreamer(tokenizer)\n","        _ = model.generate(\n","            input_ids=inputs,\n","            streamer=streamer,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=False,\n","            temperature=0.0,\n","            top_p=1.0,\n","            top_k=1,\n","            repetition_penalty=1.1,\n","            use_cache=True,\n","            pad_token_id=tokenizer.eos_token_id,\n","        )\n","    else:\n","        # Sinh ra k·∫øt qu·∫£ ho√†n ch·ªânh\n","        outputs = model.generate(\n","            input_ids=inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=False,\n","            temperature=0.0,\n","            top_p=1.0,\n","            top_k=1,\n","            repetition_penalty=1.1,\n","            use_cache=True,\n","            pad_token_id=tokenizer.eos_token_id,\n","        )\n","\n","        # Gi·∫£i m√£ k·∫øt qu·∫£\n","        result = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n","\n","        # C·∫Øt ph·∫ßn prompt n·∫øu c·∫ßn\n","        response = result.split(question.strip())[-1].strip()\n","        return response\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:33:08.370073Z","iopub.execute_input":"2025-06-29T12:33:08.370911Z","iopub.status.idle":"2025-06-29T12:33:08.669407Z","shell.execute_reply.started":"2025-06-29T12:33:08.370882Z","shell.execute_reply":"2025-06-29T12:33:08.668720Z"},"id":"o9HWA83PvcS4","outputId":"055a43c1-3838-4a90-e6e7-7c15acc85955"},"outputs":[{"name":"stderr","text":"Unsloth: Will map <|im_end|> to EOS = <|im_end|>.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["response = chat(\"If $x - y = X and $x + y = 12$, what is the value of $x$? If we know the answer to the above question is 9, what is the value of unknown variable X?\")\n","print(response) # ƒê√∫ng"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:36:36.474491Z","iopub.execute_input":"2025-06-29T12:36:36.474785Z","iopub.status.idle":"2025-06-29T12:36:50.658758Z","shell.execute_reply.started":"2025-06-29T12:36:36.474766Z","shell.execute_reply":"2025-06-29T12:36:50.657815Z"},"id":"Iy0iQ-gsvcS4","outputId":"fd612a95-e76f-495e-adcf-d435c0895bdb"},"outputs":[{"name":"stdout","text":"<|im_start|>assistant\nWe are given that $x - y = X$ and $x + y = 12$.\nTo find the value of $x$, we can solve this system of equations.\nAdding the two equations together, we get:\n$(x - y) + (x + y) = X + 12$\nSimplifying, we have:\n$2x = X + 12$\nDividing both sides by 2, we find:\n$x = \\frac{X + 12}{2}$\nGiven that the value of $x$ is 9, we can substitute it into the equation:\n$9 = \\frac{X + 12}{2}$\nMultiplying both sides of the equation by 2 to eliminate the fraction, we get:\n$18 = X + 12$\nSubtracting 12 from both sides of the equation, we find:\n$6 = X$\nThe value of X is 6.\nThe answer is: 6\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["chat(\"A radio show plays for 3 hours a day. They split their show into talking segments, ad breaks and songs. Talking segments last 10 minutes each, ad breaks last 5 minutes each and songs are played throughout the rest of the show. If the radio show includes 3 talking segments and x ad breaks in today‚Äôs show, how long, in minutes, does the show play songs? If we know the answer to the above question is 125, what is the value of unknown variable x?\", stream=True)\n","# ƒê√∫ng"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:45:34.342863Z","iopub.execute_input":"2025-06-29T12:45:34.343161Z","iopub.status.idle":"2025-06-29T12:45:51.049041Z","shell.execute_reply.started":"2025-06-29T12:45:34.343141Z","shell.execute_reply":"2025-06-29T12:45:51.048023Z"},"id":"S39ROoEivcS5","outputId":"743db340-68ee-440b-e6f2-f860efa9a0eb"},"outputs":[{"name":"stdout","text":"<|im_start|>user\nA radio show plays for 3 hours a day. They split their show into talking segments, ad breaks and songs. Talking segments last 10 minutes each, ad breaks last 5 minutes each and songs are played throughout the rest of the show. If the radio show includes 3 talking segments and x ad breaks in today‚Äôs show, how long, in minutes, does the show play songs? If we know the answer to the above question is 125, what is the value of unknown variable x?<|im_end|>\n<|im_start|>assistant\nThe radio show plays for 3 hours a day, which is equal to 3 * 60 = 180 minutes.\nTalking segments last 10 minutes each, so 3 talking segments will last 3 * 10 = 30 minutes.\nAd breaks last 5 minutes each, so x ad breaks will last x * 5 minutes.\nThe remaining time is the total duration minus the time spent on talking segments and ad breaks: 180 - 30 - (x * 5) minutes.\nWe are given that the show plays songs for 125 minutes, so we can write: 180 - 30 - (x * 5) = 125.\nSimplifying the left side, we get: 150 - (x * 5) = 125.\nSubtracting 150 from both sides, we get: -(x * 5) = -25.\nDividing both sides by -5, we get: x = 5.\nThe value of x is 5.\n#### 5\nThe answer is: 5<|im_end|>\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["response = chat(\"In a charity race to raise money for hurricane victims, thirty students participated. Ten of them raised $20 each, while the remaining students raised $30 each. What is the total amount of money raised by the students in the race?\")\n","print(response)\n","# ƒê√∫ng"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:45:54.457797Z","iopub.execute_input":"2025-06-29T12:45:54.458342Z","iopub.status.idle":"2025-06-29T12:46:01.040280Z","shell.execute_reply.started":"2025-06-29T12:45:54.458317Z","shell.execute_reply":"2025-06-29T12:46:01.039651Z"},"id":"SfpB6QLFvcS6","outputId":"4baaaa27-fe59-41af-cd43-02f65d843ebb"},"outputs":[{"name":"stdout","text":"<|im_start|>assistant\nThe ten students who raised $20 each contributed a total of 10 * $20 = $200.\nThe remaining twenty students who raised $30 each contributed a total of 20 * $30 = $600.\nTherefore, the total amount of money raised by the students in the race is $200 + $600 = $800.\n#### 800\nThe answer is: 800\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["chat(\"Which two-digit positive integer is one more than a multiple of 2, 3, 4, 5, and 6?\", stream=True)\n","# Sai, ƒë√°p l√†: 61 (Gi·∫£i ƒë√∫ng ƒëc 1 ph·∫ßn)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:46:11.600685Z","iopub.execute_input":"2025-06-29T12:46:11.600940Z","iopub.status.idle":"2025-06-29T12:46:28.123023Z","shell.execute_reply.started":"2025-06-29T12:46:11.600923Z","shell.execute_reply":"2025-06-29T12:46:28.122483Z"},"id":"rqlVY8rcvcS7","outputId":"e8e388d1-128a-4f9a-8a3b-b4147066db7f"},"outputs":[{"name":"stdout","text":"<|im_start|>user\nWhich two-digit positive integer is one more than a multiple of 2, 3, 4, 5, and 6?<|im_end|>\n<|im_start|>assistant\nThe least common multiple (LCM) of 2, 3, 4, 5, and 6 is 60.\nTo find the two-digit positive integer that is one more than a multiple of 60, we add 1 to each multiple of 60 until we get a two-digit number.\nThe first multiple of 60 is 60, so we add 1 to get 61.\nThe next multiple of 60 is 120, so we add 1 to get 121.\nThe next multiple of 60 is 180, so we add 1 to get 181.\nThe next multiple of 60 is 240, so we add 1 to get 241.\nThe next multiple of 60 is 300, so we add 1 to get 301.\nTherefore, the two-digit positive integer that is one more than a multiple of 2, 3, 4, 5, and 6 is $\\boxed{301}$.The answer is: 301<|im_end|>\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["### B∆∞·ªõc 8: L∆∞u m√¥ h√¨nh"],"metadata":{"id":"F24u_9t9vcS7"}},{"cell_type":"markdown","source":["#### T·∫£i m√¥ h√¨nh"],"metadata":{"id":"SdozWF2pvcTB"}},{"cell_type":"code","source":["model.save_pretrained(\"mistral-metamathqa-lora-100k-v1\")\n","tokenizer.save_pretrained(\"mistral-metamathqa-lora-100k-v1\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:38:58.720046Z","iopub.execute_input":"2025-06-29T12:38:58.720841Z","iopub.status.idle":"2025-06-29T12:38:59.524653Z","shell.execute_reply.started":"2025-06-29T12:38:58.720813Z","shell.execute_reply":"2025-06-29T12:38:59.524011Z"},"id":"7SdxHzfbvcTD","outputId":"91d45c8a-314e-481c-ef31-fa1e138f71d2"},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('mistral-metamathqa-lora-100k-v1/tokenizer_config.json',\n 'mistral-metamathqa-lora-100k-v1/special_tokens_map.json',\n 'mistral-metamathqa-lora-100k-v1/tokenizer.json')"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["import shutil\n","\n","shutil.make_archive(\n","    base_name=\"/kaggle/working/mistral-metamathqa-lora-100k-v1\",  # T√™n file zip (kh√¥ng c·∫ßn .zip ·ªü ƒë√¢y)\n","    format=\"zip\",  # C√≥ th·ªÉ l√†: 'zip', 'tar', 'gztar', 'bztar', 'xztar'\n","    root_dir=\"/kaggle/working/mistral-metamathqa-lora-100k-v1\"\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:39:02.480116Z","iopub.execute_input":"2025-06-29T12:39:02.480405Z","iopub.status.idle":"2025-06-29T12:39:19.557030Z","shell.execute_reply.started":"2025-06-29T12:39:02.480386Z","shell.execute_reply":"2025-06-29T12:39:19.556388Z"},"id":"xVJpoXjBvcTE","outputId":"7ae6b1db-3343-41f9-eb41-a08dfb6ea07f"},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/mistral-metamathqa-lora-100k-v1.zip'"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["### Load m√¥ h√¨nh l√™n HuggingFace"],"metadata":{"id":"132sZixEvcTE"}},{"cell_type":"code","source":["from huggingface_hub import create_repo\n","\n","create_repo(\"mistral-metamathqa-lora-100k-v1\", private=False)  # ho·∫∑c private=False n·∫øu mu·ªën c√¥ng khai"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:39:36.303665Z","iopub.execute_input":"2025-06-29T12:39:36.303943Z","iopub.status.idle":"2025-06-29T12:39:36.799068Z","shell.execute_reply.started":"2025-06-29T12:39:36.303922Z","shell.execute_reply":"2025-06-29T12:39:36.798524Z"},"id":"2yass62jvcTF","outputId":"29d819c9-dfe5-4033-8b18-0b28251762c0"},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/hahuy2004/mistral-metamathqa-lora-100k-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='hahuy2004/mistral-metamathqa-lora-100k-v1')"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","api = HfApi()\n","api.upload_folder(\n","    folder_path=\"/kaggle/working/mistral-metamathqa-lora-100k-v1\",\n","    repo_id=\"hahuy2004/mistral-metamathqa-lora-100k-v1\",  # s·ª≠a l·∫°i ƒë√∫ng username Hugging Face c·ªßa b·∫°n\n","    repo_type=\"model\"\n",")\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T12:39:39.451526Z","iopub.execute_input":"2025-06-29T12:39:39.452236Z","iopub.status.idle":"2025-06-29T12:39:50.270505Z","shell.execute_reply.started":"2025-06-29T12:39:39.452204Z","shell.execute_reply":"2025-06-29T12:39:50.269884Z"},"colab":{"referenced_widgets":["2dcc001cee4141e4a0c05f50c13b4c8f"]},"id":"YceC7QbRvcTF","outputId":"8431f07e-9ae8-4acb-ea3d-7e25834c9657"},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dcc001cee4141e4a0c05f50c13b4c8f"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/hahuy2004/mistral-metamathqa-lora-100k-v1/commit/955deeeaa7cb90a096a5b5c36c5c833a7bbe98f1', commit_message='Upload folder using huggingface_hub', commit_description='', oid='955deeeaa7cb90a096a5b5c36c5c833a7bbe98f1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hahuy2004/mistral-metamathqa-lora-100k-v1', endpoint='https://huggingface.co', repo_type='model', repo_id='hahuy2004/mistral-metamathqa-lora-100k-v1'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":null}]}